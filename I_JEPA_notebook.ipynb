{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5bad29f",
   "metadata": {},
   "source": [
    "# I-JEPA Tutorial Notebook (Standard Setup, CIFAR-10)\n",
    "# --------------------------------------------------\n",
    "# This notebook follows the *original I-JEPA design philosophy*:\n",
    "# - Context encoder (student)\n",
    "# - Target encoder (teacher, EMA)\n",
    "# - No reconstruction loss\n",
    "# - Predict latent representations of masked target regions\n",
    "# - No negative samples, no contrastive loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ecc72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# 1. Imports & setup\n",
    "# ==================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from timm.models.vision_transformer import VisionTransformer\n",
    "from tqdm import tqdm\n",
    "import math, random, os\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc723bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# 2. Hyperparameters (CIFAR-friendly, I-JEPA style)\n",
    "# ==================================================\n",
    "IMG_SIZE = 32\n",
    "PATCH = 8                     # 4x4 grid => 16 patches (important for blocks)\n",
    "NUM_PATCHES = (IMG_SIZE // PATCH) ** 2\n",
    "\n",
    "CTX_RATIO = 0.5               # fraction of patches kept as context\n",
    "CTX_KEEP = int(CTX_RATIO * NUM_PATCHES)\n",
    "\n",
    "EMB_DIM = 384\n",
    "DEPTH_ENC = 6\n",
    "DEPTH_PRED = 3\n",
    "HEADS = 6\n",
    "MLP_RATIO = 4\n",
    "\n",
    "BATCH = 512\n",
    "EPOCHS = 90\n",
    "BASE_LR = 1.5e-4\n",
    "EMA_MOMENTUM = 0.996\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faa87c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# 3. Dataset (CIFAR-10)\n",
    "# ==================================================\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.6, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                         (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "train_set = datasets.CIFAR10(root='.', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH, shuffle=True,\n",
    "                          drop_last=True, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515f9c5a",
   "metadata": {},
   "source": [
    "# Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1183f136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# 4. Encoders\n",
    "# ==================================================\n",
    "class IJEPAViT(nn.Module):\n",
    "    \"\"\"\n",
    "    ViT backbone without class token.\n",
    "    Outputs patch-level embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.vit = VisionTransformer(\n",
    "            img_size=IMG_SIZE,\n",
    "            patch_size=PATCH,\n",
    "            in_chans=3,\n",
    "            embed_dim=EMB_DIM,\n",
    "            depth=DEPTH_ENC,\n",
    "            num_heads=HEADS,\n",
    "            mlp_ratio=MLP_RATIO,\n",
    "            num_classes=0,\n",
    "            global_pool=''\n",
    "        )\n",
    "\n",
    "    def forward(self, x, patch_idx=None):\n",
    "        x = self.vit.patch_embed(x)              # B, N, D\n",
    "        x = x + self.vit.pos_embed[:, 1:, :]\n",
    "        B, N, D = x.shape\n",
    "\n",
    "        if patch_idx is not None:\n",
    "            x = x[torch.arange(B)[:, None], patch_idx]\n",
    "\n",
    "        x = self.vit.blocks(x)\n",
    "        x = self.vit.norm(x)\n",
    "        return x                                 # B, n_patches, D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73adccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# 5. Predictor (latent-space prediction head)\n",
    "# ==================================================\n",
    "class Predictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, EMB_DIM))\n",
    "        self.blocks = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=EMB_DIM,\n",
    "                nhead=HEADS,\n",
    "                dim_feedforward=EMB_DIM * MLP_RATIO,\n",
    "                batch_first=True\n",
    "            ) for _ in range(DEPTH_PRED)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(EMB_DIM)\n",
    "\n",
    "    def forward(self, context, queries):\n",
    "        x = torch.cat([context, queries], dim=1)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        return self.norm(x[:, -queries.shape[1]:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2feffb09",
   "metadata": {},
   "source": [
    "# Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0915bc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# 6. Mask sampling (I-JEPA-style)\n",
    "# ==================================================\n",
    "\n",
    "def sample_context_patches(B, N, keep):\n",
    "    return torch.stack([\n",
    "        torch.randperm(N, device=device)[:keep]\n",
    "        for _ in range(B)\n",
    "    ])\n",
    "\n",
    "\n",
    "def sample_target_patches(B, N, num_targets=4):\n",
    "    return torch.stack([\n",
    "        torch.randperm(N, device=device)[:num_targets]\n",
    "        for _ in range(B)\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ddd23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# 7. Model instantiation\n",
    "# ==================================================\n",
    "context_encoder = IJEPAViT().to(device)\n",
    "target_encoder = IJEPAViT().to(device)\n",
    "predictor = Predictor().to(device)\n",
    "\n",
    "# Initialize target encoder = context encoder\n",
    "with torch.no_grad():\n",
    "    for ps, pt in zip(context_encoder.parameters(), target_encoder.parameters()):\n",
    "        pt.copy_(ps)\n",
    "        pt.requires_grad = False\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    list(context_encoder.parameters()) + list(predictor.parameters()),\n",
    "    lr=BASE_LR, weight_decay=0.05\n",
    ")\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d691d6",
   "metadata": {},
   "source": [
    "# Training loop (90 epochs, cosine LR, EMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c90e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# 8. Training loop (I-JEPA loss)\n",
    "# ==================================================\n",
    "\n",
    "def cosine_schedule(step, total_steps):\n",
    "    return 0.5 * (1 + math.cos(math.pi * step / total_steps))\n",
    "\n",
    "steps_per_epoch = len(train_loader)\n",
    "total_steps = steps_per_epoch * EPOCHS\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}\")\n",
    "\n",
    "    for it, (x, _) in enumerate(pbar):\n",
    "        x = x.to(device)\n",
    "        B = x.size(0)\n",
    "\n",
    "        ctx_idx = sample_context_patches(B, NUM_PATCHES, CTX_KEEP)\n",
    "        tgt_idx = sample_target_patches(B, NUM_PATCHES, num_targets=4)\n",
    "\n",
    "        # Context encoding\n",
    "        z_ctx = context_encoder(x, ctx_idx)\n",
    "\n",
    "        # Target encoding (EMA, no grad)\n",
    "        with torch.no_grad():\n",
    "            z_tgt = target_encoder(x)\n",
    "            z_tgt = z_tgt[torch.arange(B)[:, None], tgt_idx]\n",
    "\n",
    "        # Predictor\n",
    "        queries = predictor.mask_token.expand(B, tgt_idx.size(1), -1)\n",
    "        z_pred = predictor(z_ctx, queries)\n",
    "\n",
    "        # I-JEPA loss (cosine similarity)\n",
    "        z_pred = F.normalize(z_pred, dim=-1)\n",
    "        z_tgt = F.normalize(z_tgt, dim=-1)\n",
    "        loss = 2 - 2 * (z_pred * z_tgt).sum(dim=-1).mean()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # EMA update\n",
    "        with torch.no_grad():\n",
    "            step = epoch * steps_per_epoch + it\n",
    "            m = 1 - (1 - EMA_MOMENTUM) * cosine_schedule(step, total_steps)\n",
    "            for ps, pt in zip(context_encoder.parameters(), target_encoder.parameters()):\n",
    "                pt.mul_(m).add_((1 - m) * ps)\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | Avg Loss: {running_loss / steps_per_epoch:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfd81fd",
   "metadata": {},
   "source": [
    "# Linear evaluation (frozen target encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "225c10e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear probe accuracy: 0.5142\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def extract_features(loader, encoder):\n",
    "#     feats, labels = [], []\n",
    "#     for x, y in loader:\n",
    "#         x = x.to(device)\n",
    "#         #z = encoder(x).mean(dim=1)\n",
    "#         z = encoder(x); z = z.reshape(B,4,4,-1).mean([1,2])\n",
    "#         feats.append(F.normalize(z, dim=1).cpu())\n",
    "#         labels.append(y)\n",
    "#     return torch.cat(feats), torch.cat(labels)\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_features(loader, encoder):\n",
    "    feats, labels = [], []\n",
    "    for x, y in loader:\n",
    "        x = x.to(device)\n",
    "        z = encoder(x)                      # B, 16, 384\n",
    "        z = z.reshape(z.size(0), 4, 4, -1)  # 4x4 spatial grid\n",
    "        z = z.mean(dim=[1, 2])              # spatial average pooling\n",
    "        feats.append(F.normalize(z, dim=1).cpu())\n",
    "        labels.append(y)\n",
    "    return torch.cat(feats), torch.cat(labels)\n",
    "\n",
    "train_loader_eval = DataLoader(datasets.CIFAR10('.', train=True, transform=transform),\n",
    "                               batch_size=500, shuffle=False)\n",
    "test_loader_eval  = DataLoader(datasets.CIFAR10('.', train=False, transform=transform),\n",
    "                               batch_size=500, shuffle=False)\n",
    "\n",
    "z_tr, y_tr = extract_features(train_loader_eval, target_encoder)\n",
    "z_te, y_te = extract_features(test_loader_eval, target_encoder)\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000, solver='lbfgs', n_jobs=-1)\n",
    "clf.fit(z_tr, y_tr)\n",
    "print(\"Linear probe accuracy:\", clf.score(z_te, y_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cf7fbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
